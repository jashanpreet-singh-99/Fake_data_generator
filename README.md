# Fake_data_generator

This project entails the generation of a synthetic dataset that adheres to specific constraints as per the requirements. The synthesized dataset serves as the training data for a machine learning model. Following the completion of the model training, a thorough verification process is conducted to ascertain whether the model has successfully acquired the desired constraints or not.

## Table of Contents

- [Introduction](#introduction)
- [Installation](#installation)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)

## Introduction

This project involves the creation of a synthetic dataset that conforms to predetermined constraints, as dictated by the project requirements. The process begins by generating a dataset that encompasses the necessary characteristics and limitations outlined for the project. This synthesized dataset is subsequently utilized as the training data for a machine learning model.

Once the dataset is prepared, the machine learning model undergoes an extensive training phase. During this phase, the model learns from the synthetic dataset, gradually acquiring knowledge and patterns from the provided data. The training process aims to optimize the model's performance and enable it to make accurate predictions or classifications based on the learned constraints.

After the model training is completed, a meticulous verification procedure is carried out. The purpose of this verification is to evaluate whether the model has effectively internalized and comprehended the desired constraints. Various techniques and metrics are employed to assess the model's performance, including comparisons against ground truth data or evaluation against predefined benchmarks.

By conducting this verification step, the project team can ensure that the machine learning model has successfully acquired the specified constraints and is capable of applying them accurately to new, unseen data. This verification phase serves as a critical quality control measure, validating the reliability and efficacy of the trained model before it is deployed in real-world applications.

## Installation

1. Clone the repository to your local machine.
2. Make sure you have the required dependencies installed: pandas, numpy, scikit-learn.
3. Upload the required database files to the notebook.

## Usage

1. Update the file paths in the code to match the names and locations of your local database files.
2. Load the data frames from the specified paths.
3. Adjust column names if needed.
4. Transform literals to numerical values.
5. Split the data into training and testing sets.
6. Train the model using logistic regression.
7. Evaluate the accuracy of the model on the training data and individual test datasets.
8. Customize the code and experiment with different models or techniques as needed.

## Contributing

If you'd like to contribute to this project, please follow these steps:

1. Fork the repository.
2. Create a new branch for your feature or bug fix.
3. Make your changes and commit them with descriptive commit messages.
4. Push your changes to your forked repository.
5. Submit a pull request, explaining your changes and their purpose.

## License

This project is licensed under the MIT License.

